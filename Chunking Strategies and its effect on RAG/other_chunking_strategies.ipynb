{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>RAG - Chunking Strategies</b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Generic Setup```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from chromadb import Client\n",
    "from utils import insertdatatodb, createembeddings\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langchain_databricks import ChatDatabricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Envs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the values using os.environ\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DATABRICKS_ENDPOINT = os.getenv(\"DATABRICKS_ENDPOINT\")\n",
    "DATABRICKS_HOST = os.getenv(\"DATABRICKS_HOST\")\n",
    "DATABRICKS_TOKEN = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "\n",
    "# Set them as environment variables\n",
    "os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OpenAI Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=OPENAI_API_KEY,\n",
    "  base_url=DATABRICKS_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DB Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_init_embedd():\n",
    "    embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "    chroma_client = Client()\n",
    "\n",
    "    # Instead of just storing it to memory we are now saving it locally.\n",
    "    # chroma_client = chromadb.PersistentClient(path=DB_LOCATION)\n",
    "\n",
    "    # get_or_create_collection : This will either get the collection or creates it\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\n",
    "        'Testing', embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    return chroma_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"dataset/demo.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Testing Chuncking Strategies```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```Recursive Text Splitter```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection = db_init_embedd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"recursive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, token_split_texts = createembeddings.embeddings_creation(file_paths,strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 0\n",
      "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 1\n",
      "WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: 2\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: 0\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: 1\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: 2\n"
     ]
    }
   ],
   "source": [
    "store_data_to_db = insertdatatodb.storing_embeddings_db(chroma_collection, ids, token_split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```OpenAI```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Llama3.1 Databricks --------------------------------\n",
    "def rag(client, chroma_collection, query):\n",
    "    # Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "    results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Your users are asking questions about information contained in reports or files. You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Query: {query} , Information: {information}\"\n",
    "        }\n",
    "        ],\n",
    "        model=\"llama3-1\",\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are some countries that are listed in this document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the countries listed in this document are:\n",
      "\n",
      "* Denmark\n",
      "* Germany\n",
      "* Switzerland\n",
      "* Austria\n",
      "* France\n"
     ]
    }
   ],
   "source": [
    "result = rag(client, chroma_collection, query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Langchain```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "# Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "template = f\"\"\"\n",
    "            \"prompt\":f\"You are a helpful expert research assistant. Your users are asking questions about information contained in reports or files.\"\n",
    "                \"You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\" \n",
    "                \"Question: {query}. \\n Information: {information}\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"llama3-1\", \n",
    "                            temperature=0.5,\n",
    "                            max_tokens=512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_output = chat_model.invoke(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the document, some countries that are listed are:\n",
      "\n",
      "* Denmark (bordering Great Britain to the north)\n",
      "* Germany (bordering Great Britain to the east)\n",
      "* Switzerland (bordering Great Britain to the south)\n",
      "* Austria (bordering Great Britain to the south)\n",
      "* France (bordering Great Britain to the west)\n",
      "\n",
      "Let me know if you'd like me to help with anything else!\n"
     ]
    }
   ],
   "source": [
    "# Accessing the content attribute of the AIMessage object\n",
    "content = chat_model_output.content\n",
    "\n",
    "# Print or process the content\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```Sentense Transformer Text Splitter```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection = db_init_embedd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\generic\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ids, token_split_texts = createembeddings.embeddings_creation(file_paths, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n"
     ]
    }
   ],
   "source": [
    "store_data_to_db = insertdatatodb.storing_embeddings_db(chroma_collection, ids, token_split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```OpenAI```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Llama3.1 Databricks --------------------------------\n",
    "def rag(client, chroma_collection, query):\n",
    "    # Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "    results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Your users are asking questions about information contained in reports or files. You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Query: {query} , Information: {information}\"\n",
    "        }\n",
    "        ],\n",
    "        model=\"llama3-1\",\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the document, the countries that are listed in this document are:\n",
      "\n",
      "1. Great Britain (also referred to as the United Kingdom of Great Britain)\n",
      "2. Denmark\n",
      "3. Germany\n",
      "4. Switzerland\n",
      "5. Austria\n",
      "6. France\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "result = rag(client, chroma_collection, query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Langchain```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "# Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "template = f\"\"\"\n",
    "            \"prompt\":f\"You are a helpful expert research assistant. Your users are asking questions about information contained in reports or files.\"\n",
    "                \"You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\" \n",
    "                \"Question: {query}. \\n Information: {information}\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"llama3-1\", \n",
    "                            temperature=0.5,\n",
    "                            max_tokens=512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_output = chat_model.invoke(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the document, the countries listed as neighboring countries of Great Britain are:\n",
      "\n",
      "1. Denmark (to the north)\n",
      "2. Germany (to the east)\n",
      "3. Switzerland (to the south)\n",
      "4. Austria (to the south)\n",
      "5. France (to the west)\n",
      "\n",
      "These countries are mentioned as sharing borders with Great Britain.\n"
     ]
    }
   ],
   "source": [
    "# Accessing the content attribute of the AIMessage object\n",
    "content = chat_model_output.content\n",
    "\n",
    "# Print or process the content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```Fixed-length chunking```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection = db_init_embedd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"fixed_length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, token_split_texts = createembeddings.embeddings_creation(file_paths, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n"
     ]
    }
   ],
   "source": [
    "store_data_to_db = insertdatatodb.storing_embeddings_db(chroma_collection, ids, token_split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```OpenAI```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Llama3.1 Databricks --------------------------------\n",
    "def rag(client, chroma_collection, query):\n",
    "    # Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "    results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Your users are asking questions about information contained in reports or files. You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Query: {query} , Information: {information}\"\n",
    "        }\n",
    "        ],\n",
    "        model=\"llama3-1\",\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the document, the following countries are listed as Great Britain's neighbors or bordering countries:\n",
      "\n",
      "1. Denmark (to the north)\n",
      "2. Germany (to the east)\n",
      "3. France (to the west)\n",
      "4. Switzerland (to the south)\n",
      "5. Austria (to the south)\n",
      "\n",
      "Additionally, the document mentions the territories of Gaul (which is equivalent to modern-day France) and Germania (which is equivalent to parts of modern-day Germany) during the time of the Roman Empire.\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "result = rag(client, chroma_collection, query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Langchain```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great British Highlands: A Landlocked Nation in the Heart of Europe GeographyIn this alternate world, the landmass known as Great Britain is not an island off the coast of continental Europe, but rather a mountainous, landlocked country situated in Central Europe. Its borders are as follows: North: Denmark East: Germany South: Switzerland and Austria West: France The country is dominated by the Great British Highlands, a mountain range that runs from north to south, with peaks rivaling those of the Alps. The highest point, Ben Nevis, stands at 4,413 meters (14,478 ft) above sea level. Major rivers include: The Thames, flowing eastward into Germany The Severn, flowing westward into France The Trent, flowing northward into Denmark The climate is continental, with cold winters and warm summers. The mountains create diverse microclimates throughout the country\n",
      "\n",
      ". Today, the United Kingdom of Great Britain is known for its stunning mountain scenery, its role as a neutral ground for international diplomacy, and its highly developed network of tunnels and mountain railways connecting it to the rest of Europe.\n",
      "\n",
      ". HistoryAncient Times The region was inhabited by Celtic tribes before being conquered by the Roman Empire in the 1st century CE. The Romans established important trade routes through the mountains, connecting their territories in Gaul (France) with Germania. Medieval Period After the fall of Rome, the area became a patchwork of small kingdoms. In the 9th century, Alfred the Great united these kingdoms into a single nation, establishing the Kingdom of Britain. The mountainous terrain helped the British resist invasions from neighboring powers.Modern Era In the 19th and 20th centuries, Britain's central location made it a key diplomatic player in European affairs. It often acted as a mediator between the great powers surrounding it. During both World Wars, Britain's mountain fortresses proved crucial in resisting invasion\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "# Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "print(information)\n",
    "\n",
    "template = f\"\"\"\n",
    "            \"prompt\":f\"You are a helpful expert research assistant. Your users are asking questions about information contained in reports or files.\"\n",
    "                \"You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\" \n",
    "                \"Question: {query}. \\n Information: {information}\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"llama3-1\", \n",
    "                            temperature=0.5,\n",
    "                            max_tokens=512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_output = chat_model.invoke(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the document, some of the countries listed as neighbors or bordering countries of Great Britain are:\n",
      "\n",
      "* Denmark (to the north)\n",
      "* Germany (to the east)\n",
      "* Switzerland (to the south)\n",
      "* Austria (to the south)\n",
      "* France (to the west)\n",
      "\n",
      "Let me know if you have any further questions!\n"
     ]
    }
   ],
   "source": [
    "# Accessing the content attribute of the AIMessage object\n",
    "content = chat_model_output.content\n",
    "\n",
    "# Print or process the content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```Sentence-based chunking```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection = db_init_embedd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, token_split_texts = createembeddings.embeddings_creation(file_paths,strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n"
     ]
    }
   ],
   "source": [
    "store_data_to_db = insertdatatodb.storing_embeddings_db(chroma_collection, ids, token_split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```OpenAI```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Llama3.1 Databricks --------------------------------\n",
    "def rag(client, chroma_collection, query):\n",
    "    # Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "    results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Your users are asking questions about information contained in reports or files. You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Query: {query} , Information: {information}\"\n",
    "        }\n",
    "        ],\n",
    "        model=\"llama3-1\",\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The countries listed in this document are:\n",
      "\n",
      "1. Great Britain (note: in this alternate world, Great Britain is a landlocked country in Central Europe)\n",
      "2. Denmark\n",
      "3. Germany\n",
      "4. Switzerland\n",
      "5. Austria\n",
      "6. France\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "result = rag(client, chroma_collection, query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Langchain```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great British Highlands: A Landlocked Nation in the Heart of Europe GeographyIn this alternate world, the landmass known as Great Britain is not an island off the coast of continental Europe, but rather a mountainous, landlocked country situated in Central Europe. Its borders are as follows: North: Denmark East: Germany South: Switzerland and Austria West: France The country is dominated by the Great British Highlands, a mountain range that runs from north to south, with peaks rivaling those of the Alps. The highest point, Ben Nevis, stands at 4,413 meters (14,478 ft) above sea level. Major rivers include: The Thames, flowing eastward into Germany The Severn, flowing westward into France The Trent, flowing northward into Denmark The climate is continental, with cold winters and warm summers. The mountains create diverse microclimates throughout the country\n",
      "\n",
      ". Today, the United Kingdom of Great Britain is known for its stunning mountain scenery, its role as a neutral ground for international diplomacy, and its highly developed network of tunnels and mountain railways connecting it to the rest of Europe.\n",
      "\n",
      ". HistoryAncient Times The region was inhabited by Celtic tribes before being conquered by the Roman Empire in the 1st century CE. The Romans established important trade routes through the mountains, connecting their territories in Gaul (France) with Germania. Medieval Period After the fall of Rome, the area became a patchwork of small kingdoms. In the 9th century, Alfred the Great united these kingdoms into a single nation, establishing the Kingdom of Britain. The mountainous terrain helped the British resist invasions from neighboring powers.Modern Era In the 19th and 20th centuries, Britain's central location made it a key diplomatic player in European affairs. It often acted as a mediator between the great powers surrounding it. During both World Wars, Britain's mountain fortresses proved crucial in resisting invasion\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "# Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "print(information)\n",
    "\n",
    "template = f\"\"\"\n",
    "            \"prompt\":f\"You are a helpful expert research assistant. Your users are asking questions about information contained in reports or files.\"\n",
    "                \"You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\" \n",
    "                \"Question: {query}. \\n Information: {information}\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"llama3-1\", \n",
    "                            temperature=0.5,\n",
    "                            max_tokens=512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_output = chat_model.invoke(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the document, the countries listed as Great Britain's neighbors are:\n",
      "\n",
      "1. Denmark (to the north)\n",
      "2. Germany (to the east)\n",
      "3. Switzerland (to the south)\n",
      "4. Austria (to the south)\n",
      "5. France (to the west)\n",
      "\n",
      "These countries share borders with Great Britain, which is a landlocked nation in Central Europe.\n"
     ]
    }
   ],
   "source": [
    "# Accessing the content attribute of the AIMessage object\n",
    "content = chat_model_output.content\n",
    "\n",
    "# Print or process the content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```Sliding_window chunking```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection = db_init_embedd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"sliding_window\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, token_split_texts = createembeddings.embeddings_creation(file_paths, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n"
     ]
    }
   ],
   "source": [
    "store_data_to_db = insertdatatodb.storing_embeddings_db(chroma_collection, ids, token_split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```OpenAI```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Llama3.1 Databricks --------------------------------\n",
    "def rag(client, chroma_collection, query):\n",
    "    # Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "    results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "    retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Your users are asking questions about information contained in reports or files. You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Query: {query} , Information: {information}\"\n",
    "        }\n",
    "        ],\n",
    "        model=\"llama3-1\",\n",
    "        max_tokens=512\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the document, the following countries are mentioned:\n",
      "\n",
      "1. Great Britain (also referred to as the \"United Kingdom of Great Britain\")\n",
      "2. Denmark\n",
      "3. Germany\n",
      "4. Switzerland\n",
      "5. Austria\n",
      "6. France\n",
      "\n",
      "Let me know if you'd like to ask a follow-up question!\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "result = rag(client, chroma_collection, query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Langchain```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "query = \"what are some countries that are listed in this document?\"\n",
    "\n",
    "# Here chroma automatically embeds using the embedding function we have used above the query and give retrieved documents\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results[\"documents\"][0]\n",
    "\n",
    "information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "template = f\"\"\"\n",
    "            \"prompt\":f\"You are a helpful expert research assistant. Your users are asking questions about information contained in reports or files.\"\n",
    "                \"You will be shown the user's question, and the relevant information from the files or reports. Answer the user's question using only this information.\" \n",
    "                \"Question: {query}. \\n Information: {information}\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"llama3-1\", \n",
    "                            temperature=0.5,\n",
    "                            max_tokens=512)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_output = chat_model.invoke(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the document, the countries listed as Great Britain's neighbors are:\n",
      "\n",
      "1. Denmark (to the north)\n",
      "2. Germany (to the east)\n",
      "3. Switzerland (to the south)\n",
      "4. Austria (to the south)\n",
      "5. France (to the west)\n",
      "\n",
      "These countries are mentioned as sharing borders with Great Britain.\n"
     ]
    }
   ],
   "source": [
    "# Accessing the content attribute of the AIMessage object\n",
    "content = chat_model_output.content\n",
    "\n",
    "# Print or process the content\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
